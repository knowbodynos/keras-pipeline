# Learning rate
lr: 0.004

# Loss (cost) function
loss: 'mean_squared_error'

# Other metrics
metrics:
  - 'mean_absolute_error'
  - 'mean_absolute_percentage_error'

# Optimizer parameters
opt: 
  # name: sgd
  # momentum: 0.3
  # nesterov: False
  name: adam
  beta_1: 0.9
  beta_2: 0.99
  eps_coeff: 1
  eps_exp: -8
  amsgrad: False

# Regularizers
kernel_reg: 
  l1: 0.001
  l2: 0.
bias_reg: 
  l1: 0.001
  l2: 0.

# Training parameters
batch_size: 50
epochs: 800
verbosity: 2
train_split: 0.8
rand_seed: 7
val_split: 0.1
shuffle: True
dropout: 0.1

# Hidden layers
# units -> n - 2, acts -> n - 1, norms -> n - 1, dropouts -> n - 1
hidden: 
  # Hidden layer sizes
  # If n(i) -> n(i+1) is an EQL layer, then n(i) = u + 2v and n(i+1) = u + v.
  # Then, v = n(i) - n(i+1) and u = n(i+1) - v = 2n(i+1) - n(i).
  # Then, for both u > 0 and v > 0, we need n(i+1) < n(i) < 2n(i+1).
  # EQL layers: out < in < 2 * out
  - layer: 1
    in_units: 
    out_units: 50
    act: sigmoid
    norm: True
    dropout: True

  # - layer: 2
  #   in_units: 50
  #   out_units: 30
  #   act:
  #     eql: square_act
  #   norm: False
  #   dropout: False

  - layer: 3
    in_units: 50
    out_units: 100
    act: lin_act
    norm: True
    dropout: True

  # - layer: 4
  #   in_units: 10
  #   out_units: 15
  #   act:
  #     eql: cubic_act
  #   norm: False
  #   dropout: False

  # - layer: 5
  #   in_units: 15
  #   out_units: 30
  #   act: lin_act
  #   norm: True
  #   dropout: True

  - layer: 4
    in_units: 100
    out_units: 70
    act:
      eql: lin_act
    norm: False
    dropout: False

  # - layer: 5
  #   in_units: 70
  #   out_units: 10
  #   act: lin_act
  #   norm: True
  #   dropout: True

  - layer: 6
    in_units: 70
    out_units: 
    act: sigmoid
    norm: False
    dropout: False

  # - layer: 7
  #   in_units: 50
  #   out_units: 
  #   act: relu
  #   norm: False
  #   dropout: False

scheduler:
  crit_epochs: 
    - 200
    - 400
    - 600
  drop_factors:
    - 2
    - 2
    - 2